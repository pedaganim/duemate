name: Deploy to AWS

on:
  push:
    branches:
      - main
      - staging
      - develop
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        type: choice
        default: 'dev'
        options:
          - dev
          - staging
          - production
      terraform_action:
        description: 'Terraform action to perform'
        required: true
        type: choice
        default: 'apply'
        options:
          - plan
          - apply
          - destroy

env:
  NODE_VERSION: '20.x'
  TERRAFORM_VERSION: '1.6.0'

permissions:
  id-token: write
  contents: read

jobs:
  # Validate that all required secrets and variables are set
  validate-secrets:
    name: Validate Required Secrets
    runs-on: ubuntu-latest
    steps:
      - name: Check Required Secrets
        run: |
          echo "==============================================="
          echo "Validating Required Secrets and Variables"
          echo "==============================================="
          echo ""
          
          # Track if any required secrets are missing
          MISSING_SECRETS=0
          
          # Check AWS_ROLE_ARN (required for all deployments)
          if [ -z "${{ secrets.AWS_ROLE_ARN }}" ]; then
            echo "❌ ERROR: AWS_ROLE_ARN is not set"
            echo "   This secret is REQUIRED for AWS authentication via OIDC"
            echo "   Please configure it in: Settings → Secrets and variables → Actions"
            echo ""
            MISSING_SECRETS=1
          else
            echo "✅ AWS_ROLE_ARN is configured"
          fi
          
          # Check AWS_REGION (optional, has default)
          if [ -z "${{ secrets.AWS_REGION }}" ]; then
            echo "⚠️  AWS_REGION is not set (will default to 'us-east-1')"
          else
            echo "✅ AWS_REGION is configured"
          fi
          
          # Check TERRAFORM_STATE_BUCKET (optional)
          if [ -z "${{ secrets.TERRAFORM_STATE_BUCKET }}" ]; then
            echo "⚠️  TERRAFORM_STATE_BUCKET is not set (Terraform will use local state)"
          else
            echo "✅ TERRAFORM_STATE_BUCKET is configured"
          fi
          
          echo ""
          echo "==============================================="
          
          # Fail if any required secrets are missing
          if [ $MISSING_SECRETS -eq 1 ]; then
            echo ""
            echo "❌ VALIDATION FAILED: Required secrets are missing"
            echo ""
            echo "Required Secrets:"
            echo "  - AWS_ROLE_ARN (REQUIRED)"
            echo ""
            echo "Optional Secrets:"
            echo "  - AWS_REGION (defaults to 'us-east-1')"
            echo "  - TERRAFORM_STATE_BUCKET (defaults to local state)"
            echo ""
            echo "For setup instructions, see .github/SECURITY.md"
            echo ""
            exit 1
          else
            echo "✅ All required secrets are configured"
            echo ""
          fi

  # Job to determine which environment to deploy to
  determine-environment:
    name: Determine Environment
    runs-on: ubuntu-latest
    needs: [validate-secrets]
    outputs:
      environment: ${{ steps.set-env.outputs.environment }}
      terraform_action: ${{ steps.set-env.outputs.terraform_action }}
    steps:
      - name: Set Environment
        id: set-env
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "environment=${{ inputs.environment }}" >> $GITHUB_OUTPUT
            echo "terraform_action=${{ inputs.terraform_action }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "environment=production" >> $GITHUB_OUTPUT
            echo "terraform_action=apply" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" = "refs/heads/staging" ]; then
            echo "environment=staging" >> $GITHUB_OUTPUT
            echo "terraform_action=apply" >> $GITHUB_OUTPUT
          else
            echo "environment=dev" >> $GITHUB_OUTPUT
            echo "terraform_action=apply" >> $GITHUB_OUTPUT
          fi

  # Build and test the application
  build-and-test:
    name: Build and Test
    runs-on: ubuntu-latest
    needs: [validate-secrets]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build TypeScript
        run: npm run build

      - name: Run tests
        run: npm test || echo "Tests not yet implemented"

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            dist/
            node_modules/
            package.json
            package-lock.json
          retention-days: 1

  # Deploy infrastructure with Terraform
  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [determine-environment, build-and-test]
    environment: ${{ needs.determine-environment.outputs.environment }}
    outputs:
      api_endpoint: ${{ steps.terraform-output.outputs.api_endpoint }}
      frontend_url: ${{ steps.terraform-output.outputs.frontend_url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Terraform Format Check
        working-directory: ./terraform
        run: terraform fmt -check -recursive
        continue-on-error: true

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init -backend-config="bucket=${{ secrets.TERRAFORM_STATE_BUCKET }}" || terraform init

      - name: Terraform Validate
        working-directory: ./terraform
        run: terraform validate

      - name: Detect and Enable Import for Existing Resources
        working-directory: ./terraform
        run: |
          set -e
          
          echo "=============================================="
          echo "Checking for Existing AWS Resources"
          echo "=============================================="
          echo ""
          
          # Get AWS account ID once (optimization)
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text 2>/dev/null)
          
          # Read project name from terraform.tfvars if it exists, otherwise use default
          # This is more robust than parsing the variables.tf file
          if [ -f "terraform.tfvars" ] && grep -q "project_name" terraform.tfvars; then
            PROJECT_NAME=$(grep "project_name" terraform.tfvars | sed 's/.*=\s*"\(.*\)".*/\1/')
          else
            PROJECT_NAME="duemate"
          fi
          
          ENVIRONMENT="${{ needs.determine-environment.outputs.environment }}"
          
          # Check if customer_name is used (for whitelabel deployments)
          # For standard deployments without customer name, the pattern is: project-environment
          # For whitelabel deployments with customer name, the pattern is: project-customer-environment
          # We'll check both patterns to be safe
          CUSTOMER_NAME="${{ vars.CUSTOMER_NAME }}"
          
          if [ -n "$CUSTOMER_NAME" ] && [ "$CUSTOMER_NAME" != "null" ]; then
            NAME_PREFIX="${PROJECT_NAME}-${CUSTOMER_NAME}-${ENVIRONMENT}"
            echo "Whitelabel deployment detected"
            echo "Customer: ${CUSTOMER_NAME}"
          else
            NAME_PREFIX="${PROJECT_NAME}-${ENVIRONMENT}"
            echo "Standard deployment (no customer name)"
          fi
          
          echo "Project: ${PROJECT_NAME}"
          echo "Environment: ${ENVIRONMENT}"
          echo "Name Prefix: ${NAME_PREFIX}"
          echo "AWS Account: ${AWS_ACCOUNT_ID}"
          echo ""
          
          # Function to check if a resource exists
          check_resource_exists() {
            local resource_type=$1
            local resource_name=$2
            
            case $resource_type in
              "dynamodb")
                aws dynamodb describe-table --table-name "$resource_name" >/dev/null 2>&1 && return 0 || return 1
                ;;
              "s3")
                aws s3api head-bucket --bucket "$resource_name" >/dev/null 2>&1 && return 0 || return 1
                ;;
              "iam-role")
                aws iam get-role --role-name "$resource_name" >/dev/null 2>&1 && return 0 || return 1
                ;;
              "iam-policy")
                aws iam get-policy --policy-arn "arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${resource_name}" >/dev/null 2>&1 && return 0 || return 1
                ;;
            esac
          }
          
          # Check for key resources that indicate existing infrastructure
          RESOURCES_EXIST=false
          RESOURCES_CHECKED=0
          RESOURCES_FOUND=0
          
          echo "Checking DynamoDB table: ${NAME_PREFIX}-main"
          RESOURCES_CHECKED=$((RESOURCES_CHECKED + 1))
          if check_resource_exists "dynamodb" "${NAME_PREFIX}-main"; then
            echo "✓ DynamoDB table exists"
            RESOURCES_EXIST=true
            RESOURCES_FOUND=$((RESOURCES_FOUND + 1))
          else
            echo "✗ DynamoDB table does not exist"
          fi
          
          echo "Checking S3 bucket: ${NAME_PREFIX}-frontend"
          RESOURCES_CHECKED=$((RESOURCES_CHECKED + 1))
          if check_resource_exists "s3" "${NAME_PREFIX}-frontend"; then
            echo "✓ S3 frontend bucket exists"
            RESOURCES_EXIST=true
            RESOURCES_FOUND=$((RESOURCES_FOUND + 1))
          else
            echo "✗ S3 frontend bucket does not exist"
          fi
          
          echo "Checking IAM role: ${NAME_PREFIX}-lambda-execution"
          RESOURCES_CHECKED=$((RESOURCES_CHECKED + 1))
          if check_resource_exists "iam-role" "${NAME_PREFIX}-lambda-execution"; then
            echo "✓ IAM Lambda role exists"
            RESOURCES_EXIST=true
            RESOURCES_FOUND=$((RESOURCES_FOUND + 1))
          else
            echo "✗ IAM Lambda role does not exist"
          fi
          
          # Check additional resources to get better signal
          echo "Checking S3 bucket: ${NAME_PREFIX}-invoices"
          RESOURCES_CHECKED=$((RESOURCES_CHECKED + 1))
          if check_resource_exists "s3" "${NAME_PREFIX}-invoices"; then
            echo "✓ S3 invoices bucket exists"
            RESOURCES_EXIST=true
            RESOURCES_FOUND=$((RESOURCES_FOUND + 1))
          else
            echo "✗ S3 invoices bucket does not exist"
          fi
          
          echo "Checking IAM policy: ${NAME_PREFIX}-lambda-sqs"
          RESOURCES_CHECKED=$((RESOURCES_CHECKED + 1))
          if check_resource_exists "iam-policy" "${NAME_PREFIX}-lambda-sqs"; then
            echo "✓ IAM lambda-sqs policy exists"
            RESOURCES_EXIST=true
            RESOURCES_FOUND=$((RESOURCES_FOUND + 1))
          else
            echo "✗ IAM lambda-sqs policy does not exist"
          fi
          
          echo ""
          echo "Resources found: ${RESOURCES_FOUND}/${RESOURCES_CHECKED}"
          echo ""
          
          # Enable import configuration if resources exist
          if [ "$RESOURCES_EXIST" = true ]; then
            echo "=============================================="
            echo "⚠️  Existing Resources Detected"
            echo "=============================================="
            echo ""
            echo "Enabling automatic import configuration..."
            
            if [ -f "import.tf.example" ]; then
              # Generate dynamic import.tf with only resources that exist in AWS
              echo "Generating dynamic import.tf based on existing AWS resources..."
              
              # Start with the base import.tf.example
              cp import.tf.example import.tf
              
              # Check if conditional resources exist and add import blocks if needed
              # These are the resources that were removed from import.tf.example
              # Use || true to continue even if checks fail
              
              # Check for API Gateway CloudWatch role
              if aws iam get-role --role-name "${NAME_PREFIX}-api-cloudwatch-role" >/dev/null 2>&1; then
                echo "✓ Found API Gateway CloudWatch role, adding import block"
                cat >> import.tf <<'EOF' || echo "⚠️  Warning: Failed to add API Gateway CloudWatch role import block"

# API Gateway CloudWatch Role (detected in AWS)
import {
  to = module.api_gateway.aws_iam_role.api_gateway_cloudwatch[0]
  id = "${local.import_name_prefix}-api-cloudwatch-role"
}
EOF
              fi
              
              # Check for API Gateway CloudWatch log group
              if aws logs describe-log-groups --log-group-name-prefix "/aws/apigateway/${NAME_PREFIX}-api" --query 'logGroups[0].logGroupName' --output text 2>/dev/null | grep -q "^/aws/apigateway/${NAME_PREFIX}-api$"; then
                echo "✓ Found API Gateway log group, adding import block"
                cat >> import.tf <<'EOF' || echo "⚠️  Warning: Failed to add API Gateway log group import block"

# API Gateway CloudWatch Log Group (detected in AWS)
import {
  to = module.api_gateway.aws_cloudwatch_log_group.api_gateway[0]
  id = "/aws/apigateway/${local.import_name_prefix}-api"
}
EOF
              fi
              
              # Check for Cognito User Pool Domain
              # We need to check if the domain exists by looking up the user pool first
              USER_POOL_ID=$(aws cognito-idp list-user-pools --max-results 60 --query "UserPools[?Name=='${NAME_PREFIX}-users'].Id" --output text 2>/dev/null || echo "")
              if [ -n "$USER_POOL_ID" ]; then
                # Check if domain exists for this pool
                DOMAIN_EXISTS=$(aws cognito-idp describe-user-pool --user-pool-id "$USER_POOL_ID" --query 'UserPool.Domain' --output text 2>/dev/null || echo "")
                if [ -n "$DOMAIN_EXISTS" ] && [ "$DOMAIN_EXISTS" != "None" ]; then
                  echo "✓ Found Cognito User Pool Domain, adding import block"
                  cat >> import.tf <<'EOF' || echo "⚠️  Warning: Failed to add Cognito domain import block"

# Cognito User Pool Domain (detected in AWS)
import {
  to = module.cognito.aws_cognito_user_pool_domain.main
  id = "${local.import_name_prefix}-users"
}
EOF
                fi
              fi
              
              echo "✅ Import configuration enabled (import.tf)"
              echo ""
              echo "Terraform will automatically import existing resources"
              echo "to prevent 'EntityAlreadyExists' errors."
              
              # IMPORTANT: Run manual import BEFORE terraform plan
              # This ensures resources are in state before plan runs
              # 
              # Import Strategy:
              # - import-resources.sh: Handles ALL resources, including conditional ones
              #   and resources requiring dynamic AWS API lookups (Cognito)
              # - import.tf: Provides backup imports for non-conditional resources
              #   (activated during terraform plan if resources are missing from state)
              # 
              # The script may partially fail (e.g., conditional resources with count=0),
              # which is expected. It returns success for resources that don't exist in
              # AWS or in the configuration, letting Terraform create them.
              echo ""
              echo "Running manual import script to pre-populate state..."
              chmod +x import-resources.sh
              
              if [ -n "$CUSTOMER_NAME" ] && [ "$CUSTOMER_NAME" != "null" ]; then
                echo "Running: ./import-resources.sh $ENVIRONMENT $PROJECT_NAME $CUSTOMER_NAME"
                ./import-resources.sh "$ENVIRONMENT" "$PROJECT_NAME" "$CUSTOMER_NAME" || {
                  echo "⚠️  Warning: Some imports may have failed in import-resources.sh"
                  echo "Terraform will use import.tf for remaining non-conditional resources"
                  echo "Check the import script output above for details"
                }
              else
                echo "Running: ./import-resources.sh $ENVIRONMENT $PROJECT_NAME"
                ./import-resources.sh "$ENVIRONMENT" "$PROJECT_NAME" || {
                  echo "⚠️  Warning: Some imports may have failed in import-resources.sh"
                  echo "Terraform will use import.tf for remaining non-conditional resources"
                  echo "Check the import script output above for details"
                }
              fi
            else
              echo "⚠️  Warning: import.tf.example not found"
              echo "Proceeding without import configuration"
            fi
          else
            echo "=============================================="
            echo "✅ No Existing Resources Detected"
            echo "=============================================="
            echo ""
            echo "This appears to be a new deployment."
            echo "Terraform will create all resources from scratch."
            
            # Ensure import.tf is not present for new deployments
            if [ -f "import.tf" ]; then
              echo ""
              echo "Removing import.tf to prevent import failures..."
              rm -f import.tf
            fi
          fi
          
          echo ""

      - name: Terraform Plan
        working-directory: ./terraform
        run: |
          set -e
          set -o pipefail
          
          echo "=============================================="
          echo "Running Terraform Plan"
          echo "Environment: ${{ needs.determine-environment.outputs.environment }}"
          echo "Region: ${{ vars.AWS_REGION || 'us-east-1' }}"
          echo "=============================================="
          echo ""
          
          # Run terraform plan and capture all output (stdout and stderr)
          if ! terraform plan \
            -var="environment=${{ needs.determine-environment.outputs.environment }}" \
            -var="aws_region=${{ vars.AWS_REGION || 'us-east-1' }}" \
            -out=tfplan 2>&1; then
            
            echo ""
            echo "=============================================="
            echo "❌ Terraform Plan Failed"
            echo "=============================================="
            echo ""
            echo "Please review the error output above for details."
            echo ""
            echo "Common issues:"
            echo "  - Missing or invalid AWS credentials"
            echo "  - Missing required variables"
            echo "  - Resource conflicts or naming issues"
            echo "  - Invalid Terraform configuration"
            echo ""
            
            # Show current state if available for additional context
            echo "Current Terraform state (if available):"
            terraform show 2>/dev/null || echo "No state file found (new deployment or state backend issue)"
            echo ""
            
            exit 1
          fi
          
          echo ""
          echo "✅ Terraform plan completed successfully"

      - name: Terraform Apply
        if: needs.determine-environment.outputs.terraform_action == 'apply'
        working-directory: ./terraform
        run: |
          set -e
          set -o pipefail
          
          echo "=============================================="
          echo "Running Terraform Apply"
          echo "=============================================="
          echo ""
          
          if [ ! -f tfplan ]; then
            echo "❌ ERROR: tfplan file not found. Terraform plan failed or was skipped."
            exit 1
          fi
          
          # Define error patterns for resource conflicts
          # Using word boundaries to ensure exact matches and avoid false positives
          RESOURCE_EXISTS_PATTERN="\b(EntityAlreadyExists|BucketAlreadyExists|ResourceAlreadyExists|ResourceInUseException)\b"
          ERROR_PREFIX_PATTERN="Error: creating.*already exists"
          
          # Run terraform apply and capture output to a file for analysis
          if ! terraform apply -auto-approve tfplan 2>&1 | tee /tmp/terraform_apply.log; then
            echo ""
            echo "=============================================="
            echo "❌ Terraform Apply Failed"
            echo "=============================================="
            echo ""
            
            # Check if the error is due to resources already existing
            if grep -qE "${RESOURCE_EXISTS_PATTERN}" /tmp/terraform_apply.log; then
              echo "⚠️  DETECTED: Resources already exist in AWS"
              echo ""
              echo "This error occurs when AWS resources exist but are not in Terraform state."
              echo ""
              echo "Troubleshooting steps:"
              echo "1. The automatic import detection should have handled this"
              echo "2. Check if the resource names match the expected pattern"
              echo "3. Verify that import.tf was properly enabled (check earlier logs)"
              echo "4. Manual fix: Use the import-resources.sh script:"
              echo "   cd terraform"
              echo "   ./import-resources.sh ${{ needs.determine-environment.outputs.environment }}"
              echo "   terraform plan"
              echo "   terraform apply"
              echo ""
              echo "Resources that failed to import:"
              # Show both error patterns separately for clarity
              grep -E "${ERROR_PREFIX_PATTERN}" /tmp/terraform_apply.log || true
              grep -E "${RESOURCE_EXISTS_PATTERN}" /tmp/terraform_apply.log || true
              echo ""
            fi
            
            echo "Please review the error output above for details."
            echo ""
            echo "The infrastructure may be in a partial state."
            echo "Review the Terraform state and error messages carefully."
            echo ""
            exit 1
          fi
          
          echo ""
          echo "✅ Terraform apply completed successfully"

      - name: Terraform Destroy
        if: needs.determine-environment.outputs.terraform_action == 'destroy'
        working-directory: ./terraform
        run: |
          set -e
          set -o pipefail
          
          echo "=============================================="
          echo "Running Terraform Destroy"
          echo "Environment: ${{ needs.determine-environment.outputs.environment }}"
          echo "Region: ${{ vars.AWS_REGION || 'us-east-1' }}"
          echo "=============================================="
          echo ""
          echo "⚠️  WARNING: This will destroy all infrastructure!"
          echo ""
          
          # Run terraform destroy and capture all output (stdout and stderr)
          if ! terraform destroy -auto-approve \
            -var="environment=${{ needs.determine-environment.outputs.environment }}" \
            -var="aws_region=${{ vars.AWS_REGION || 'us-east-1' }}" 2>&1; then
            
            echo ""
            echo "=============================================="
            echo "❌ Terraform Destroy Failed"
            echo "=============================================="
            echo ""
            echo "Please review the error output above for details."
            echo ""
            echo "Some resources may still exist. Check the AWS console"
            echo "and Terraform state to verify what was deleted."
            echo ""
            exit 1
          fi
          
          echo ""
          echo "✅ Terraform destroy completed successfully"

      - name: Get Terraform Outputs
        if: needs.determine-environment.outputs.terraform_action != 'destroy'
        id: terraform-output
        working-directory: ./terraform
        run: |
          API_ENDPOINT=$(terraform output -raw api_gateway_endpoint 2>/dev/null || echo "")
          FRONTEND_URL=$(terraform output -raw frontend_url 2>/dev/null || echo "")
          echo "api_endpoint=${API_ENDPOINT}" >> $GITHUB_OUTPUT
          echo "frontend_url=${FRONTEND_URL}" >> $GITHUB_OUTPUT

  # Deploy application code
  deploy-application:
    name: Deploy Application
    runs-on: ubuntu-latest
    needs: [determine-environment, build-and-test, deploy-infrastructure]
    if: needs.determine-environment.outputs.terraform_action != 'destroy'
    environment: ${{ needs.determine-environment.outputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Deploy Lambda Functions
        run: |
          chmod +x ./scripts/deploy-lambda.sh
          ./scripts/deploy-lambda.sh ${{ needs.determine-environment.outputs.environment }}

      - name: Verify Database Setup
        run: |
          chmod +x ./scripts/run-migrations.sh
          ./scripts/run-migrations.sh ${{ needs.determine-environment.outputs.environment }}

  # Deploy frontend (if applicable)
  deploy-frontend:
    name: Deploy Frontend
    runs-on: ubuntu-latest
    needs: [determine-environment, deploy-infrastructure]
    if: needs.determine-environment.outputs.terraform_action != 'destroy'
    environment: ${{ needs.determine-environment.outputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Deploy Frontend to S3
        if: hashFiles('frontend/**') != ''
        run: |
          if [ -d "frontend" ]; then
            chmod +x ./scripts/deploy-frontend.sh
            ./scripts/deploy-frontend.sh ${{ needs.determine-environment.outputs.environment }}
          else
            echo "No frontend directory found, skipping frontend deployment"
          fi

  # Post-deployment verification
  verify-deployment:
    name: Verify Deployment
    runs-on: ubuntu-latest
    needs: [determine-environment, deploy-application, deploy-infrastructure]
    if: needs.determine-environment.outputs.terraform_action != 'destroy'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Verify Lambda Functions
        run: |
          chmod +x ./scripts/verify-deployment.sh
          ./scripts/verify-deployment.sh ${{ needs.determine-environment.outputs.environment }}

      - name: Run Smoke Tests
        if: needs.deploy-infrastructure.outputs.api_endpoint != ''
        run: |
          API_ENDPOINT="${{ needs.deploy-infrastructure.outputs.api_endpoint }}"
          echo "Testing API endpoint: $API_ENDPOINT"

          # Test health endpoint
          curl -f "$API_ENDPOINT/health" || echo "Health check endpoint not available yet"

          # Test API documentation
          curl -f "$API_ENDPOINT/api-docs" || echo "API docs endpoint not available yet"

      - name: Deployment Summary
        run: |
          echo "## Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ needs.determine-environment.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Region:** ${{ vars.AWS_REGION || 'us-east-1' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -n "${{ needs.deploy-infrastructure.outputs.api_endpoint }}" ]; then
            echo "**API Endpoint:** ${{ needs.deploy-infrastructure.outputs.api_endpoint }}" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -n "${{ needs.deploy-infrastructure.outputs.frontend_url }}" ]; then
            echo "**Frontend URL:** ${{ needs.deploy-infrastructure.outputs.frontend_url }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ Deployment completed successfully!" >> $GITHUB_STEP_SUMMARY
